---
sidebar_position: 998
---

!!! warning "本ページの情報は古くなっており（DSPy 2.5および2.6バージョンでは）完全に正確ではない可能性があります"


# よくある質問

## DSPyは私に適したツールでしょうか？ DSPyと他のフレームワークとの比較

**DSPy**の設計思想と抽象化レベルは他のライブラリやフレームワークと大きく異なるため、特定の用途において**DSPy**が適切なフレームワークかどうかを判断するのは比較的容易です。自然言語処理/AIの研究者（あるいは新たな処理パイプラインやタスクを模索している実務者）であれば、その答えは通常明白な**Yes**となります。それ以外の用途で活用されている実務者の方は、引き続きお読みください。

**DSPy vs. プロンプト用簡易ラッパー（OpenAI API、MiniChain、基本的なテンプレート処理）** 言い換えれば： _なぜプロンプトを直接文字列テンプレートとして記述できないのでしょうか？_ 極めて単純な設定であれば、この方法でも十分機能する場合があります（ニューラルネットワークに精通している方なら、これは2層の小規模NNをPythonのforループで表現するようなもので、ある程度は動作します）。しかし、より高品質な出力や管理可能なコストを求める場合、多段階の分解処理、プロンプト設計の改善、データのブートストラッピング、慎重なファインチューニング、検索拡張技術の活用、あるいはより小規模な（あるいは低コストな、あるいはローカル環境の）モデルの利用を検討する必要があります。基盤モデルを用いた構築の真の表現力は、これらの要素間の相互作用にあります。ただし、これらの要素のいずれかを変更するたびに、他の複数のコンポーネントに影響を及ぼし（あるいは機能を低下させる）可能性があります。**DSPy**は、これらの相互作用のうちシステム設計の外部に位置する部分を、明確に抽象化し（かつ強力に最適化します）。実際のシステム設計に集中できるよう、モジュールレベルの相互作用の設計に専念できるようにします。つまり、**DSPy**では10～20行程度の簡潔なコードで表現できる同じプログラムが、`GPT-4`向けの多段階指示文、`Llama2-13b`向けの詳細なプロンプト、あるいは`T5-base`向けのファインチューニング設定として容易にコンパイル可能なのです。さらに、プロジェクトの中核に長く脆弱でモデル固有の文字列を維持する必要もなくなります。

**DSPyとLangChain/LlamaIndexなどのアプリケーション開発ライブラリとの比較** LangChainとLlamaIndexは高レベルなアプリケーション開発を対象としている。これらは事前に構築されたアプリケーションモジュールを提供しており、ユーザーは自身のデータや設定環境に簡単に組み込むことができる。PDF文書や標準的なテキストからSQLへの変換など、汎用的なプロンプトで質問応答を行いたい場合、これらのライブラリには豊富なエコシステムが存在する。**DSPy**は特定のアプリケーション向けに手作業で作成されたプロンプトを内部に備えていない。代わりに、**DSPy**はより強力で汎用的なモジュール群を導入しており、これらはユーザーのデータを用いてパイプライン内でLMにプロンプトを生成させる（あるいはファインチューニングを行う）ことができる。データの変更、プログラムの制御フローの調整、対象とするLMの変更などがあった場合、**DSPyコンパイラ**はこのパイプラインに最適化された新たなプロンプトセット（あるいはファインチューニング済みモデル）を生成することが可能である。このため、**DSPy**はユーザーが独自の短いプログラムを実装（あるいは拡張）する意欲があれば、最小限の労力でタスク最高水準の品質を達成できる可能性がある。要するに、**DSPy**は軽量でありながら自動的に最適化を行うプログラミングモデルを必要とする場合に適した選択肢であり、事前に定義されたプロンプトや統合機能のライブラリとは異なるものである。ニューラルネットワークに精通している読者向けに説明すると、これはPyTorch（すなわち**DSPy**を表す）とHuggingFace Transformers（すなわち高レベルライブラリを表す）の違いに相当する。

**DSPyとGuidance/LMQL/RELM/Outlinesなどの生成制御ライブラリとの比較** これらはいずれも、LMの個別の出力生成を制御するための革新的なライブラリ群である。例えば、JSON出力スキーマの強制適用や、特定の正規表現に基づくサンプリング制限などを実現することができる。これらは多くの場面で非常に有用であるが、一般的には単一のLM呼び出しに対する低レベルかつ構造化された制御に焦点を当てている。そのため、これらのライブラリでは、得られるJSON出力（あるいは構造化出力）がタスクにとって実際に正しいものであるか、または有用であるかを保証するものではない。対照的に、**DSPy**はプログラム内のプロンプトを自動的に最適化し、様々なタスク要件に適合させる機能を備えており、これには有効な構造化出力の生成も含まれる。ただし、**DSPy**ではこれらのライブラリが実装する正規表現風の制約を表現するための**Signatures**機能の導入を検討中である。

## 基本的な使用方法

**自分のタスクにDSPyをどのように活用すればよいか？** この件については[8ステップのガイド](learn/index.md)を作成している。簡潔に説明すると、DSPyの使用は反復的なプロセスである。まずタスクを定義し、最大化したい評価指標を設定した上で、いくつかのサンプル入力を用意する。通常、これらの入力にはラベルを付与しない（ただし、評価指標が最終出力に対するラベルを必要とする場合は例外である）。次に、組み込みのレイヤー（`モジュール`）を選択してパイプラインを構築し、各レイヤーに`signature`（入力/出力仕様）を指定する。その後、Pythonコード内でこれらのモジュールを自由に呼び出してパイプラインを構築する。最後に、**DSPyオプティマイザ**を使用してコードをコンパイルし、高品質な指示文、自動few-shot例、あるいはLM用の更新済み重みを生成する。

**複雑なプロンプトをDSPyパイプラインに変換する方法は？** 上記の回答をご参照ください。

**DSPyオプティマイザーは何を最適化するのか？** あるいは「コンパイル処理は実際にどのような動作をするのか？」各オプティマイザーによって最適化対象は異なりますが、いずれもプログラムの評価指標を最大化することを目的としており、プロンプトや言語モデルのパラメータを更新することでこれを実現します。現在利用可能なDSPyの`optimizers`は、以下の機能を備えています：データの分析、プログラム実行時のトレースシミュレーションによる各処理ステップの好事例・悪事例の生成、過去の結果に基づく各ステップの指示内容の提案・改良、自己生成データを用いた言語モデルのパラメータファインチューニング、あるいはこれらの手法を複数組み合わせることで品質向上やコスト削減を図ることが可能です。私たちは、さらに広範な最適化空間を探索する新規オプティマイザーの統合を積極的に検討しています。現在手動で実施しているプロンプトエンジニアリングの各工程、「合成データ」生成、あるいは自己改善プロセスなどは、おそらく任意の言語モデルプログラムに適用可能なDSPyオプティマイザーとして一般化できる可能性が高いと考えています。

その他のよくある質問。これらの質問に対する正式な回答を追加するためのPRを歓迎します。既存のIssue、チュートリアル、またはこれらのトピックに関する論文のいずれかに回答が記載されている場合があります。

- **複数の出力を得るにはどうすればよいか？**

複数の出力フィールドを指定することが可能です。簡潔な形式のシグネチャでは、"->"記号に続いてカンマ区切りで複数の出力を列挙できます（例："inputs -> output1, output2"）。詳細な形式のシグネチャでは、`dspy.OutputField`を複数指定することができます。


- **独自の評価指標を定義することは可能か？ 評価指標は浮動小数点数を返すことができるか？**

評価指標は、モデルの生成結果を処理し、ユーザーが定義した評価基準に基づいて評価を行うシンプルなPython関数として定義可能です。評価指標は、既存データ（例えば正解ラベル）とモデルの予測結果を比較する場合や、言語モデル（LLM-as-Judgesなど）からの検証フィードバックを用いて出力の各種要素を評価する場合などに使用できます。評価指標は`bool`型、`int`型、および`float`型のスコアを返すことが可能です。カスタム評価指標の定義方法や、AIフィードバックやDSPyプログラムを活用した高度な評価手法については、公式ドキュメント[評価指標ガイド](learn/evaluation/metrics.md)をご覧ください。

- **コンパイル処理のコストや処理速度はどのくらいか？**

コンパイル処理のコストを示すため、参考事例として[BootstrapFewShotWithRandomSearch](api/optimizers/BootstrapFewShotWithRandomSearch.md)オプティマイザーを使用し、`gpt-3.5-turbo-1106`モデル上で7つの候補プログラムと10スレッドを用いてコンパイルを行った実験結果を報告します。このプログラムのコンパイルには約6分を要し、API呼び出し回数は3200回、入力トークン数は270万、出力トークン数は15.6万でした。この場合の総コストはOpenAIモデルの現行価格で3米ドルと算出されています。

DSPyの`optimizers`をコンパイルする際には当然ながら追加の言語モデル呼び出しが発生しますが、私たちはこのオーバーヘッドを最小限に抑えつつ実行効率を最大化する設計を採用しています。これにより、小規模モデルの性能向上を図るため、大規模モデルを用いてDSPyプログラムをコンパイルし、コンパイル時に学習した高度な動作パターンを、推論時に対象の小規模モデルに継承させるといった新たな可能性が開かれます。


## デプロイメントまたは再現性に関する懸念事項

- **コンパイル済みプログラムのチェックポイントをどのように保存すればよいですか？**

以下に、コンパイル済みモジュールの保存/読み込み方法の具体例を示します：

```python
cot_compiled = teleprompter.compile(CoT(), trainset=trainset, valset=devset)

# 保存処理
cot_compiled.save('compiled_cot_gsm8k.json')

# 読み込み処理:
cot = CoT()
cot.load('compiled_cot_gsm8k.json')
```

- **デプロイ用のエクスポート方法を教えてください**

DSPyプログラムのエクスポートは、前述の方法で保存するだけで完了します！

- **自分のデータを検索する方法は？**

[RAGautouille](https://github.com/bclavie/ragatouille)などのオープンソースライブラリを使用すれば、ColBERTなどの高度な検索モデルを用いて、文書の埋め込みやインデックス作成を行うツール経由で自身のデータを検索することが可能です。DSPyプログラムの開発と並行して、このようなライブラリを組み込むことで、検索可能なデータセットを簡単に作成できます！

- **キャッシュを無効化する方法とキャッシュをエクスポートする方法は？**

v2.5以降では、`dspy.LM`モジュールの`cache`パラメータを`False`に設定することで、キャッシュを無効化できます：

```python
dspy.LM('openai/gpt-4o-mini',  cache=False)
```

ローカルキャッシュは、グローバル環境ディレクトリ `os.environ["DSP_CACHEDIR"]` またはノートブック用の `os.environ["DSP_NOTEBOOK_CACHEDIR"]` に保存されます。通常、キャッシュディレクトリは `os.path.join(repo_path, 'cache')` に設定し、この場所からキャッシュをエクスポートできます：
```python
os.environ["DSP_NOTEBOOK_CACHEDIR"] = os.path.join(os.getcwd(), 'cache')
```

!!! warning "重要"
    `DSP_CACHEDIR`は旧バージョンのクライアント（dspy.OpenAI、dspy.ColBERTv2など）を担当し、`DSPY_CACHEDIR`は新バージョンのdspy.LMクライアントを担当します。

    AWS Lambda環境でデプロイする場合、DSP_\*およびDSPY_\*の両方を無効化する必要があります。


## 高度な使用方法

- **並列処理を行うにはどうすればよいですか？**
DSpyプログラムのコンパイル時および評価時の両方で並列処理を行うには、各DSpy `optimizer`で複数のスレッド設定を指定するか、`dspy.Evaluate`ユーティリティ関数内で設定を行います。

- **モジュールを凍結するにはどうすればよいですか？**

モジュールを凍結するには、その`._compiled`属性をTrueに設定します。これにより、モジュールが最適化処理を通過済みであり、パラメータを調整すべきでないことが示されます。この処理は`dspy.BootstrapFewShot`などの最適化器内で内部的に行われます。具体的には、教師プログラムがブートストラッピング処理中に収集した少数ショットデモンストレーションを伝播する前に、学生プログラムが確実に凍結されるように制御されます。

- **DSpyのアサーション機能をどのように使用すればよいですか？**

    a) **プログラムにアサーションを追加する方法**:
    - **制約条件の定義**: `dspy.Assert`および/または`dspy.Suggest`を使用して、DSpyプログラム内に制約条件を定義します。これらは、強制したい出力結果に対する論理検証チェックに基づいており、単純なPython関数でモデル出力を検証することも可能です。
    - **アサーションの統合**: アサーション文はモデルの生成処理の後（ヒント: モジュール層の後）に配置してください。

    b) **アサーションを有効化する方法**:
    1. **`assert_transform_module`の使用**:
        - `assert_transform_module`関数を使用して、アサーションを含むDSpyモジュールをラップします。この関数には`backtrack_handler`も指定します。この関数はプログラム内に内部的なアサーションのバックトラッキングと再試行ロジックを組み込むもので、カスタマイズも可能です。
        `program_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)`
    2. **アサーションの有効化**:
        - アサーションを含むDSpyプログラムに対して直接`activate_assertions`メソッドを呼び出します: `program_with_assertions = ProgramWithAssertions().activate_assertions()`

    **注意**: アサーションを正しく使用するには、上記のいずれかの方法で`dspy.Assert`または`dspy.Suggest`ステートメントを含むDSpyプログラムを**有効化**する必要があります。

## エラー処理

- **"コンテキストが長すぎます"エラーへの対処方法は？**

DSpyで"コンテキストが長すぎます"エラーが発生している場合、プロンプト内にデモンストレーションを含めるためにDSpy最適化器を使用しており、現在のコンテキストウィンドウを超えてしまっている可能性があります。これらのパラメータ（例: `max_bootstrapped_demos`および`max_labeled_demos`）を削減することをお試しください。さらに、取得するパッセージ/ドキュメント/埋め込みの数を減らすことで、プロンプトがモデルのコンテキスト長内に収まるようにすることも可能です。

より一般的な解決策としては、LMリクエストに指定する`max_tokens`の値を増やすことです（例: `lm = dspy.OpenAI(model = ..., max_tokens = ...`)。

## ログ出力レベルの設定
DSPyは[ロギングライブラリ](https://docs.python.org/3/library/logging.html)を使用してログ出力を行います。DSPyコードのデバッグを行う場合は、以下のサンプルコードのようにログレベルをDEBUGに設定してください。

```python
import logging
logging.getLogger("dspy").setLevel(logging.DEBUG)
```

ログ量を削減したい場合は、ログレベルを WARNING または ERROR に設定してください。

```python
import logging
logging.getLogger("dspy").setLevel(logging.WARNING)
```